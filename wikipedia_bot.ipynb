{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "# ‚úÖ LangChain memory integration\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "\n",
        "# üîê Your API key\n",
        "GOOGLE_API_KEY = \"Enter your key here\"\n",
        "# you can get a key from this link gien below\n",
        "# https://aistudio.google.com/apikey\n",
        "\n",
        "\n",
        "\n",
        "# üß† Instruction for the bot\n",
        "instruction = \"You are a helpful assistant. Answer factual and general knowledge questions clearly.\"\n",
        "\n",
        "# Initialize Gemini client directly\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# üìö Optional Wikipedia lookup tool\n",
        "def wikipedia_lookup(query: str) -> str:\n",
        "    import wikipedia\n",
        "    try:\n",
        "        return wikipedia.summary(query, sentences=3)\n",
        "    except Exception as e:\n",
        "        return f\"Could not fetch info: {e}\"\n",
        "\n",
        "# Initialize LangChain memory to store chat messages\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "# Create Gemini chat session\n",
        "chat = client.chats.create(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        system_instruction=instruction,\n",
        "        tools=[wikipedia_lookup]\n",
        "    )\n",
        ")\n",
        "\n",
        "# ‚úÖ Main Q&A loop\n",
        "while True:\n",
        "    user_input = input(\"Ask a question (or type 'exit'): \").strip()\n",
        "    if user_input.lower() in ['exit', 'quit']:\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        # Store user input in LangChain memory\n",
        "        memory.chat_memory.add_user_message(user_input)\n",
        "\n",
        "        # Send input to Gemini\n",
        "        response = chat.send_message(user_input)\n",
        "        print(\"\\nüîç Answer:\", response.text, \"\\n\")\n",
        "\n",
        "        # Store bot response in memory\n",
        "        memory.chat_memory.add_ai_message(response.text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k1adhM3_Hvl",
        "outputId": "ba81d7a5-a1dd-43a5-a975-7e3edc781bf0"
      },
      "execution_count": 64,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask a question (or type 'exit'): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uPwRvzh_C6KC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}